{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_data=np.load(\"data_train.pkl\",allow_pickle=True)\n",
    "test_data=np.load(\"data_test.pkl\",allow_pickle=True)\n",
    "\n",
    "train_data=train_data\n",
    "test_data=test_data\n",
    "train_df=pd.DataFrame({'comments':train_data[0],'labels':train_data[1]})\n",
    "test_df=pd.DataFrame({'comments':test_data})\n",
    "#print(train_df)\n",
    "\n",
    "#lowercasing all comments\n",
    "train_df['comments']=train_df['comments'].str.lower() \n",
    "test_df['comments']=test_df['comments'].str.lower() \n",
    "\n",
    "train_df['sentence_token']=train_df.comments.apply(sent_tokenize)\n",
    "train_df['sentence_count']=train_df.sentence_token.apply(len)\n",
    "test_df['sentence_token']=test_df.comments.apply(sent_tokenize)\n",
    "test_df['sentence_count']=test_df.sentence_token.apply(len)\n",
    "\n",
    "\n",
    "#average word length\n",
    "def avg_word(sentence):\n",
    "    words = sentence.split()\n",
    "    return (sum(len(word) for word in words)/len(words))\n",
    "\n",
    "train_df['ave_word_len']=train_df.comments.apply(avg_word)\n",
    "test_df['ave_word_len']=test_df.comments.apply(avg_word)\n",
    "\n",
    "#remove special character\n",
    "def remove_special_char(x):\n",
    "    return re.sub(\"[^0-9a-zA-Z]+\",' ',x)\n",
    "\n",
    "train_df['comments']=train_df['comments'].apply(remove_special_char)\n",
    "test_df['comments']=test_df['comments'].apply(remove_special_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    result = \"\"\n",
    "    for word in x.split():\n",
    "        if word not in stopWords:\n",
    "            result += word+\" \"\n",
    "    return result\n",
    "\n",
    "def count_stopwords(x):\n",
    "    result = 0\n",
    "    for word in x.split():\n",
    "        if word in stopWords:\n",
    "            result += 1\n",
    "    return result\n",
    "\n",
    "\n",
    "train_df['stopwords_count']=train_df['comments'].apply(count_stopwords)\n",
    "train_df['comments']=train_df['comments'].apply(remove_stopwords)\n",
    "\n",
    "test_df['stopwords_count']=test_df['comments'].apply(count_stopwords)\n",
    "test_df['comments']=test_df['comments'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(x):\n",
    "    result = \"\"\n",
    "    for word in x.split(\" \"):\n",
    "        result+=lemmatizer.lemmatize(word)+\" \"\n",
    "    return result\n",
    "\n",
    "train_df['comments']=train_df['comments'].apply(lemmatize)\n",
    "test_df['comments']=test_df['comments'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['token']=train_df.comments.apply(word_tokenize)\n",
    "train_df['word_count']=train_df.token.apply(len)\n",
    "train_df['ave_sentence_len']=train_df['word_count']/train_df['sentence_count']\n",
    "\n",
    "test_df['token']=test_df.comments.apply(word_tokenize)\n",
    "test_df['word_count']=test_df.token.apply(len)\n",
    "test_df['ave_sentence_len']=test_df['word_count']/test_df['sentence_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comments</th>\n",
       "      <th>sentence_token</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>ave_word_len</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>token</th>\n",
       "      <th>word_count</th>\n",
       "      <th>ave_sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trout bryant led league strikeout trout actual...</td>\n",
       "      <td>[trout and bryant have both led the league in ...</td>\n",
       "      <td>4</td>\n",
       "      <td>4.689655</td>\n",
       "      <td>15</td>\n",
       "      <td>[trout, bryant, led, league, strikeout, trout,...</td>\n",
       "      <td>16</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gt like estonian good reason fear russian expa...</td>\n",
       "      <td>[&amp;gt; just like estonians have good reasons to...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>12</td>\n",
       "      <td>[gt, like, estonian, good, reason, fear, russi...</td>\n",
       "      <td>25</td>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sol primeval sotp oblivious find next episode</td>\n",
       "      <td>[will sol_primeval sotp being oblivious?, find...</td>\n",
       "      <td>2</td>\n",
       "      <td>5.272727</td>\n",
       "      <td>5</td>\n",
       "      <td>[sol, primeval, sotp, oblivious, find, next, e...</td>\n",
       "      <td>7</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>moving ostwald border back pre 1967 border dec...</td>\n",
       "      <td>[moving ostwald borders back to the pre 1967 b...</td>\n",
       "      <td>5</td>\n",
       "      <td>4.857143</td>\n",
       "      <td>71</td>\n",
       "      <td>[moving, ostwald, border, back, pre, 1967, bor...</td>\n",
       "      <td>86</td>\n",
       "      <td>17.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>take bag morty</td>\n",
       "      <td>[you have to take it out of the bag, morty!]</td>\n",
       "      <td>1</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>7</td>\n",
       "      <td>[take, bag, morty]</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            comments  \\\n",
       "0  trout bryant led league strikeout trout actual...   \n",
       "1  gt like estonian good reason fear russian expa...   \n",
       "2    sol primeval sotp oblivious find next episode     \n",
       "3  moving ostwald border back pre 1967 border dec...   \n",
       "4                                   take bag morty     \n",
       "\n",
       "                                      sentence_token  sentence_count  \\\n",
       "0  [trout and bryant have both led the league in ...               4   \n",
       "1  [&gt; just like estonians have good reasons to...               2   \n",
       "2  [will sol_primeval sotp being oblivious?, find...               2   \n",
       "3  [moving ostwald borders back to the pre 1967 b...               5   \n",
       "4       [you have to take it out of the bag, morty!]               1   \n",
       "\n",
       "   ave_word_len  stopwords_count  \\\n",
       "0      4.689655               15   \n",
       "1      5.583333               12   \n",
       "2      5.272727                5   \n",
       "3      4.857143               71   \n",
       "4      3.300000                7   \n",
       "\n",
       "                                               token  word_count  \\\n",
       "0  [trout, bryant, led, league, strikeout, trout,...          16   \n",
       "1  [gt, like, estonian, good, reason, fear, russi...          25   \n",
       "2  [sol, primeval, sotp, oblivious, find, next, e...           7   \n",
       "3  [moving, ostwald, border, back, pre, 1967, bor...          86   \n",
       "4                                 [take, bag, morty]           3   \n",
       "\n",
       "   ave_sentence_len  \n",
       "0               4.0  \n",
       "1              12.5  \n",
       "2               3.5  \n",
       "3              17.2  \n",
       "4               3.0  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, train_df.labels, test_size=0.33, random_state=0)\n",
    "\n",
    "# X_train=train_df\n",
    "# y_train=train_df.labels\n",
    "# X_test=test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntransformer = Pipeline([\\n    ('features', FeatureUnion(transformer_list=[\\n        # Part 1\\n        ('comments', Pipeline([\\n            ('selector', TextSelector('comments'))#,('tfidf', TfidfVectorizer())\\n        ])),  # booleans close\\n        #('numericals', Pipeline([\\n        #    ('selector', TypeSelector(np.number)),\\n        #    ('scaler', StandardScaler()),\\n      #  ])),  # numericals close\\n    ])),  # features close\\n])  # pipeline close\\n\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TextSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    " \n",
    "\n",
    "'''\n",
    "transformer = Pipeline([\n",
    "    ('features', FeatureUnion(transformer_list=[\n",
    "        # Part 1\n",
    "        ('comments', Pipeline([\n",
    "            ('selector', TextSelector('comments'))#,('tfidf', TfidfVectorizer())\n",
    "        ])),  # booleans close\n",
    "        #('numericals', Pipeline([\n",
    "        #    ('selector', TypeSelector(np.number)),\n",
    "        #    ('scaler', StandardScaler()),\n",
    "      #  ])),  # numericals close\n",
    "    ])),  # features close\n",
    "])  # pipeline close\n",
    "'''\n",
    "\n",
    "#transformer.fit(X_train,y_train)\n",
    "#pred = transformer.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Pipeline([\n",
    "                        ('selector',TextSelector(key='comments')),\n",
    "                        ('tfidf', TfidfVectorizer(use_idf=True, smooth_idf=1,sublinear_tf=True,norm='l2',))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 21284)\t0.1534557848387187\n",
      "  (0, 40906)\t0.23391233095307967\n",
      "  (0, 34103)\t0.11840624672860121\n",
      "  (0, 20846)\t0.12209788109706853\n",
      "  (0, 37360)\t0.16053947952497527\n",
      "  (0, 46076)\t0.16357724023745912\n",
      "  (0, 21440)\t0.25400914231967614\n",
      "  (0, 45271)\t0.14739973959839275\n",
      "  (0, 16701)\t0.1898468946534359\n",
      "  (0, 28982)\t0.2221365682407341\n",
      "  (0, 52340)\t0.28257764156606996\n",
      "  (0, 52244)\t0.2761430559106751\n",
      "  (0, 23395)\t0.3297569078541303\n",
      "  (0, 30496)\t0.368255689520253\n",
      "  (0, 41179)\t0.3946349875801853\n",
      "  (0, 6528)\t0.33533623768593446\n"
     ]
    }
   ],
   "source": [
    "print(transformer.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14360465],\n",
       "       [0.09593481],\n",
       "       [0.10195448],\n",
       "       ...,\n",
       "       [0.0869186 ],\n",
       "       [0.09853575],\n",
       "       [0.11138311]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "class NumberSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "transformer_sentence = Pipeline([\n",
    "                        ('selector',NumberSelector(key='sentence_count')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "])    \n",
    "\n",
    "transformer_sentence.fit_transform(X_train)\n",
    "\n",
    "\n",
    "transformer_word_count = Pipeline([\n",
    "                        ('selector',NumberSelector(key='word_count')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "])    \n",
    "\n",
    "transformer_stopwords_count = Pipeline([\n",
    "                        ('selector',NumberSelector(key='stopwords_count')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "])  \n",
    "\n",
    "transformer_stopwords_count = Pipeline([\n",
    "                        ('selector',NumberSelector(key='stopwords_count')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "]) \n",
    "\n",
    "transformer_ave_word_len = Pipeline([\n",
    "                        ('selector',NumberSelector(key='ave_word_len')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "]) \n",
    "\n",
    "transformer_ave_sentence_len = Pipeline([\n",
    "                        ('selector',NumberSelector(key='ave_sentence_len')),\n",
    "                        ('standard',MinMaxScaler())\n",
    "]) \n",
    "\n",
    "transformer_sentence.fit_transform(X_train)\n",
    "transformer_word_count.fit_transform(X_train)\n",
    "transformer_stopwords_count.fit_transform(X_train)\n",
    "transformer_ave_sentence_len.fit_transform(X_train)\n",
    "transformer_ave_word_len.fit_transform(X_train)\n",
    "transformer_sentence.fit_transform(X_test)\n",
    "transformer_word_count.fit_transform(X_test)\n",
    "transformer_stopwords_count.fit_transform(X_test)\n",
    "transformer_ave_sentence_len.fit_transform(X_test)\n",
    "transformer_ave_word_len.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<23100x37711 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 553594 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features=FeatureUnion([('comments',transformer)\n",
    "                              ,('sentence_count',transformer_sentence)\n",
    "                                ,('word_count',transformer_word_count)\n",
    "                                  ,('stopwords_count',transformer_word_count)\n",
    "                                  ,('ave_word_len',transformer_word_count)\n",
    "                                  ,('ave_sentence_len',transformer_word_count)])\n",
    "\n",
    "combine_features=Pipeline([('features',features)])\n",
    "combine_features.fit_transform(X_train)\n",
    "combine_features.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import SGDClassifier\n",
    "# 'clf', SGDClassifier(loss='hinge',penalty='l2',alpha=1e-3,n_iter=5,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2     accuracy:   0.550\n",
      "none     accuracy:   0.517\n",
      "l1     accuracy:   0.434\n",
      "elasticnet     accuracy:   0.538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "#l2’;其余有’none’,‘l1’,‘elasticnet’\n",
    "alpha_group = [0.00001,0.00005,0.0001,0.0002,0.0005]\n",
    "penal_group = ['l2','none','l1','elasticnet']\n",
    "#loss_group = ['hinge','log']\n",
    "for penal in penal_group:\n",
    "    clf=Pipeline([('features',features), \n",
    "                  ('classifier', SGDClassifier(loss='hinge',penalty=penal,alpha=0.00005,random_state=42))])\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "    score = np.mean(pred==y_test)\n",
    "    print(penal,\"    accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.503\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf=Pipeline([('features',features), ('classifier', SVC(kernel='linear', C=1, gamma = 1))])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "pred = clf.predict(X_test)\n",
    "score = np.mean(pred==y_test)\n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.165 \t 0.5595238095238095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# alpha_group=[0.05,0.1, 0.165, 0.2,0.25,0.3,0.4,0.5]\n",
    "alpha_group=[0.165]\n",
    "for alpha in alpha_group:\n",
    "    classifier=Pipeline([('features',features),\n",
    "                        ('classifier', MultinomialNB(alpha=alpha,fit_prior=True,class_prior=None))])\n",
    "\n",
    "    classifier.fit(X_train,y_train)\n",
    "    pred=classifier.predict(X_test)\n",
    "    print(alpha, \"\\t\" , np.mean(pred==y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline: Naive Bayes 0.5612142857142857\n",
    "                      0.5313571428571429 (minlen = 5)\n",
    "                      0.5433571428571429 (minilen =2, alpha =0.2)\n",
    "                      0.5504285714285714 (minilen =2, alpha =0.4, 1-gram + 2-gram)\n",
    "                      \n",
    "MultinomialNB minilen =1, 1-gram + 2-gram, alpha=\n",
    "0.1 \t 0.549\n",
    "0.165 \t 0.5522857142857143\n",
    "0.2 \t 0.5515714285714286\n",
    "0.25 \t 0.5502857142857143\n",
    "0.3 \t 0.5507142857142857\n",
    "0.4 \t 0.5506428571428571\n",
    "0.5 \t 0.5500714285714285"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "baseline: linear SVM 0.503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baseball' 'europe' 'anime' ... 'GlobalOffensive' 'gameofthrones' 'wow']\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "np.savetxt('predictions_phase_2.csv',np.array(pred),delimiter='\\n',newline='\\n',fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from vectorizer import TfidfVectorizer\n",
    "\n",
    "# vectorizer=TfidfVectorizer()\n",
    "# X=vectorizer.fit_transform(X_train)\n",
    "# y=y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#for n_feature in [50, 80, 100, 150, 200]:\n",
    "clf = RandomForestClassifier(n_estimators=20,max_features=100,min_impurity_decrease=0.0001,random_state=1)\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "pred = clf.predict(vectorizer.transform(X_test))\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "clf = Perceptron(max_iter=50)\n",
    " \n",
    "clf.fit(X, y)\n",
    " \n",
    "pred = clf.predict(vectorizer.transform(X_test))\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
